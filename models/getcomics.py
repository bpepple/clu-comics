"""
GetComics.org search and download functionality.
Uses cloudscraper to bypass Cloudflare protection.
"""
import cloudscraper
from bs4 import BeautifulSoup
import logging

logger = logging.getLogger(__name__)

# Create a cloudscraper instance for bypassing Cloudflare protection
# This is reused across all requests for efficiency
scraper = cloudscraper.create_scraper(
    browser={
        'browser': 'chrome',
        'platform': 'windows',
        'desktop': True
    }
)


def search_getcomics(query: str, max_pages: int = 3) -> list:
    """
    Search getcomics.org and return list of results.
    Uses cloudscraper to bypass Cloudflare protection.

    Args:
        query: Search query string
        max_pages: Maximum number of pages to search (default 3)

    Returns:
        List of dicts with keys: title, link, image
    """
    results = []
    base_url = "https://getcomics.org"

    for page in range(1, max_pages + 1):
        try:
            url = f"{base_url}/page/{page}/" if page > 1 else base_url
            params = {"s": query}

            logger.info(f"Searching getcomics.org page {page}: {query}")
            resp = scraper.get(url, params=params, timeout=30)
            resp.raise_for_status()

            soup = BeautifulSoup(resp.text, 'html.parser')

            # Find all article posts
            articles = soup.find_all("article", class_="post")
            if not articles:
                logger.info(f"No more results on page {page}")
                break

            for article in articles:
                title_el = article.find("h1", class_="post-title")
                if not title_el:
                    continue

                link_el = title_el.find("a")
                if not link_el:
                    continue

                # Get thumbnail image
                img_el = article.find("img")
                image = ""
                if img_el:
                    # Try data-src first (lazy loading), then src
                    image = img_el.get("data-lazy-src") or img_el.get("data-src") or img_el.get("src", "")

                results.append({
                    "title": title_el.get_text(strip=True),
                    "link": link_el.get("href", ""),
                    "image": image
                })

            logger.info(f"Found {len(articles)} results on page {page}")

        except Exception as e:
            logger.error(f"Error fetching/parsing page {page}: {e}")
            break

    logger.info(f"Total results found: {len(results)}")
    return results


def get_download_links(page_url: str) -> dict:
    """
    Fetch a getcomics page and extract download links.
    Uses cloudscraper to bypass Cloudflare protection.

    Args:
        page_url: URL of the getcomics page

    Returns:
        Dict with keys: pixeldrain, download_now (values are URLs or None)
        Priority: PIXELDRAIN first, then DOWNLOAD NOW
    """
    try:
        logger.info(f"Fetching download links from: {page_url}")
        resp = scraper.get(page_url, timeout=30)
        resp.raise_for_status()

        soup = BeautifulSoup(resp.text, 'html.parser')

        links = {"pixeldrain": None, "download_now": None}

        # Search for download links by title attribute
        for a in soup.find_all("a"):
            title = (a.get("title") or "").upper()
            href = a.get("href", "")

            if not href:
                continue

            if "PIXELDRAIN" in title and not links["pixeldrain"]:
                links["pixeldrain"] = href
                logger.info(f"Found PIXELDRAIN link: {href}")
            elif "DOWNLOAD NOW" in title and not links["download_now"]:
                links["download_now"] = href
                logger.info(f"Found DOWNLOAD NOW link: {href}")

        # If no links found by title, try button text content
        if not links["pixeldrain"] and not links["download_now"]:
            for a in soup.find_all("a", class_="aio-red"):
                text = a.get_text(strip=True).upper()
                href = a.get("href", "")

                if not href:
                    continue

                if "PIXELDRAIN" in text and not links["pixeldrain"]:
                    links["pixeldrain"] = href
                    logger.info(f"Found PIXELDRAIN link (by text): {href}")
                elif "DOWNLOAD" in text and not links["download_now"]:
                    links["download_now"] = href
                    logger.info(f"Found DOWNLOAD link (by text): {href}")

        return links

    except Exception as e:
        logger.error(f"Error fetching/parsing page: {e}")
        return {"pixeldrain": None, "download_now": None}


def score_getcomics_result(result_title: str, series_name: str, issue_number: str, year: int) -> int:
    """
    Score a GetComics result against wanted issue criteria.

    Scoring (max 95):
    - Series name found (all words present):      +30
    - Title tightness (few extra words):           +15
    - Issue number (#N or "Issue N"):              +30
    - Issue number (standalone bare number):       +20  (lower confidence)
    - Year matches:                                +20

    Penalties:
    - Issue range detected (collected edition):   -100  (disqualify)
    - Collected edition keywords:                  -30
    - Many extra words in title (4+):              -20

    Args:
        result_title: Title from GetComics search result
        series_name: Expected series name
        issue_number: Expected issue number (as string)
        year: Expected year (series year_began or store_date year)

    Returns:
        Score (negative scores indicate disqualification)
    """
    import re

    score = 0
    title_lower = result_title.lower()
    series_lower = series_name.lower()

    # Normalize issue number (remove leading zeros for comparison)
    issue_num = str(issue_number).lstrip('0') or '0'

    # ── DISQUALIFICATION: Issue ranges ──
    # Patterns like: "#1 – 18", "#1-18", "Issues 1-18", "#001-018"
    issue_range_patterns = [
        rf'#\d+\s*[-–—]\s*\d+',
        rf'issues?\s*\d+\s*[-–—]\s*\d+',
        rf'\(\d{{4}}\s*[-–—]\s*\d{{4}}\)',
    ]

    for range_pattern in issue_range_patterns:
        range_match = re.search(range_pattern, title_lower, re.IGNORECASE)
        if range_match:
            end_pattern = rf'[-–—]\s*0*{re.escape(issue_num)}\b'
            if re.search(end_pattern, result_title, re.IGNORECASE):
                logger.debug(f"Disqualified (issue range ending with #{issue_num}): '{range_match.group()}'")
                return -100

    # ── SERIES NAME MATCH (+30) ──
    series_words = series_lower.split()
    if all(word in title_lower for word in series_words):
        score += 30
        logger.debug(f"Series name match: +30")

    # ── TITLE TIGHTNESS (+15 bonus or -20 penalty) ──
    # Count how many "extra" words appear beyond the series name, issue, and year.
    # Titles with many extra words are likely different products.
    noise_words = {'the', 'a', 'an', 'of', 'and', 'in', 'by', 'for',
                   'to', 'from', 'with', 'on', 'at', 'or', 'is'}
    # Build expected word set using regex extraction (handles hyphens like Spider-Man)
    expected_words = set(re.findall(r'[a-z0-9]+', series_lower))
    expected_words.add(issue_num)
    if year:
        expected_words.add(str(year))
    # Common format noise that doesn't indicate a different product
    expected_words.update(['vol', 'volume', 'issue', 'comic', 'comics'])

    title_word_list = re.findall(r'[a-z0-9]+', title_lower)
    title_word_list = [w for w in title_word_list if w not in noise_words and len(w) > 1]
    expected_count = sum(1 for w in title_word_list if w in expected_words)
    extra_count = len(title_word_list) - expected_count

    if extra_count <= 1:
        score += 15
        logger.debug(f"Title tightness bonus ({extra_count} extra words): +15")
    elif extra_count >= 4:
        score -= 20
        logger.debug(f"Title tightness penalty ({extra_count} extra words): -20")

    # ── ISSUE NUMBER MATCH ──
    issue_patterns = [
        rf'#0*{re.escape(issue_num)}\b',
        rf'issue\s*0*{re.escape(issue_num)}\b',
    ]

    issue_matched = False
    for pattern in issue_patterns:
        if re.search(pattern, title_lower, re.IGNORECASE):
            score += 30
            logger.debug(f"Issue number match ({pattern}): +30")
            issue_matched = True
            break

    # Standalone number fallback (lower confidence, excludes Vol./Volume prefix)
    if not issue_matched:
        standalone_pattern = rf'\b0*{re.escape(issue_num)}\b'
        standalone_match = re.search(standalone_pattern, title_lower)
        if standalone_match:
            match_start = standalone_match.start()
            prefix = result_title[max(0, match_start - 10):match_start].lower()
            if re.search(r'[-–—]\s*$', prefix):
                logger.debug(f"Standalone number rejected (range dash)")
            elif re.search(r'\bvol(?:ume)?\.?\s*$', prefix):
                logger.debug(f"Standalone number rejected (volume prefix)")
            else:
                score += 20
                logger.debug(f"Issue number match (standalone): +20")
                issue_matched = True

    # ── YEAR MATCH (+20) ──
    if year and str(year) in result_title:
        score += 20
        logger.debug(f"Year match ({year}): +20")

    # ── COLLECTED EDITION PENALTY (-30) ──
    # Check for collection keywords outside the series name
    title_remainder = title_lower
    for word in series_words:
        title_remainder = title_remainder.replace(word, '', 1)

    collected_keywords = [
        r'\bomnibus\b',
        r'\btpb\b',
        r'\bhardcover\b',
        r'\bdeluxe\s+edition\b',
        r'\bcompendium\b',
        r'\bcomplete\s+collection\b',
        r'\blibrary\s+edition\b',
        r'\bbook\s+\d+\b',
    ]

    for kw_pattern in collected_keywords:
        if re.search(kw_pattern, title_remainder):
            score -= 30
            logger.debug(f"Collected edition penalty ({kw_pattern}): -30")
            break

    logger.debug(f"Score for '{result_title}' vs '{series_name} #{issue_number} ({year})': {score}")
    return score


#########################
#   Weekly Packs        #
#########################

def get_weekly_pack_url_for_date(pack_date: str) -> str:
    """
    Generate the GetComics weekly pack URL for a specific date.

    Args:
        pack_date: Date in YYYY.MM.DD or YYYY-MM-DD format

    Returns:
        URL string like https://getcomics.org/other-comics/2026-01-14-weekly-pack/
    """
    # Normalize date to YYYY-MM-DD format
    normalized = pack_date.replace('.', '-')
    return f"https://getcomics.org/other-comics/{normalized}-weekly-pack/"


def get_weekly_pack_dates_in_range(start_date: str, end_date: str) -> list:
    """
    Generate list of weekly pack dates between start_date and end_date.
    Weekly packs are released on Wednesdays (or Tuesdays sometimes).

    Args:
        start_date: Start date in YYYY-MM-DD format
        end_date: End date in YYYY-MM-DD format

    Returns:
        List of date strings in YYYY.MM.DD format (newest first)
    """
    from datetime import datetime, timedelta

    start = datetime.strptime(start_date, '%Y-%m-%d')
    end = datetime.strptime(end_date, '%Y-%m-%d')

    # Find all Wednesdays in the range (weekly packs typically release Wed)
    # Also include Tuesdays as some packs release then
    dates = []
    current = end

    while current >= start:
        # Check if this is a Tuesday (1) or Wednesday (2)
        if current.weekday() in [1, 2]:  # Tuesday or Wednesday
            dates.append(current.strftime('%Y.%m.%d'))
        current -= timedelta(days=1)

    return dates


def find_latest_weekly_pack_url():
    """
    Find the latest weekly pack URL from getcomics.org homepage.
    Uses cloudscraper to bypass Cloudflare protection.

    Searches the .cover-blog-posts section for links matching:
    <h2 class="post-title"><a href="...weekly-pack/">YYYY.MM.DD Weekly Pack</a></h2>

    Returns:
        Tuple of (pack_url, pack_date) or (None, None) if not found
        pack_date is in format "YYYY.MM.DD"
    """
    import re

    base_url = "https://getcomics.org"

    try:
        logger.info("Fetching getcomics.org homepage to find weekly pack")
        resp = scraper.get(base_url, timeout=30)
        resp.raise_for_status()

        soup = BeautifulSoup(resp.text, 'html.parser')

        # Find the cover-blog-posts section
        cover_section = soup.find(class_="cover-blog-posts")
        if not cover_section:
            logger.warning("Could not find .cover-blog-posts section on homepage")
            # Fall back to searching entire page
            cover_section = soup

        # Look for weekly pack links
        # Pattern: YYYY.MM.DD Weekly Pack or YYYY-MM-DD Weekly Pack
        weekly_pack_pattern = re.compile(r'(\d{4})[.\-](\d{2})[.\-](\d{2})\s*Weekly\s*Pack', re.IGNORECASE)

        for h2 in cover_section.find_all(['h2', 'h3'], class_='post-title'):
            link = h2.find('a')
            if not link:
                continue

            title = link.get_text(strip=True)
            href = link.get('href', '')

            match = weekly_pack_pattern.search(title)
            if match:
                # Found a weekly pack
                year, month, day = match.groups()
                pack_date = f"{year}.{month}.{day}"
                logger.info(f"Found weekly pack: {title} -> {href} (date: {pack_date})")
                return (href, pack_date)

        # Also check the URL pattern if title didn't match
        for link in cover_section.find_all('a', href=True):
            href = link.get('href', '')
            if 'weekly-pack' in href.lower():
                # Extract date from URL like: /other-comics/2026-01-14-weekly-pack/
                url_match = re.search(r'(\d{4})-(\d{2})-(\d{2})-weekly-pack', href, re.IGNORECASE)
                if url_match:
                    year, month, day = url_match.groups()
                    pack_date = f"{year}.{month}.{day}"
                    logger.info(f"Found weekly pack via URL: {href} (date: {pack_date})")
                    return (href, pack_date)

        logger.warning("No weekly pack found on homepage")
        return (None, None)

    except Exception as e:
        logger.error(f"Error fetching/parsing homepage for weekly pack: {e}")
        return (None, None)


def check_weekly_pack_availability(pack_url: str) -> bool:
    """
    Check if weekly pack download links are available yet.
    Uses cloudscraper to bypass Cloudflare protection.

    Returns:
        True if download links are present, False if still pending
    """
    try:
        logger.info(f"Checking weekly pack availability: {pack_url}")
        resp = scraper.get(pack_url, timeout=30)
        resp.raise_for_status()

        page_text = resp.text.lower()

        # Check for the "not ready" message
        not_ready_phrases = [
            "will be updated once all the files is complete",
            "will be updated once all the files are complete",
            "download link will be updated",
            "links will be updated"
        ]

        for phrase in not_ready_phrases:
            if phrase in page_text:
                logger.info(f"Weekly pack links not ready yet (found: '{phrase}')")
                return False

        # Check if PIXELDRAIN links exist
        soup = BeautifulSoup(resp.text, 'html.parser')
        pixeldrain_links = soup.find_all('a', href=lambda h: h and ('pixeldrain' in h.lower() or 'getcomics.org/dlds/' in h.lower()))

        if pixeldrain_links:
            logger.info(f"Weekly pack links are available ({len(pixeldrain_links)} PIXELDRAIN links found)")
            return True

        logger.info("No PIXELDRAIN links found on weekly pack page")
        return False

    except Exception as e:
        logger.error(f"Error checking pack availability: {e}")
        return False


def parse_weekly_pack_page(pack_url: str, format_preference: str, publishers: list) -> dict:
    """
    Parse a weekly pack page and extract PIXELDRAIN download links.
    Uses cloudscraper to bypass Cloudflare protection.

    Args:
        pack_url: URL of the weekly pack page
        format_preference: 'JPG' or 'WEBP'
        publishers: List of publishers to download ['DC', 'Marvel', 'Image', 'INDIE']

    Returns:
        Dict mapping publisher to pixeldrain URL: {publisher: url}
        Returns empty dict if links not yet available
    """
    import re

    result = {}

    try:
        logger.info(f"Parsing weekly pack page: {pack_url}")
        logger.info(f"Looking for format: {format_preference}, publishers: {publishers}")

        resp = scraper.get(pack_url, timeout=30)
        resp.raise_for_status()

        soup = BeautifulSoup(resp.text, 'html.parser')

        # Find the section for the requested format (JPG or WEBP)
        # Structure: <h3><span style="color: #3366ff;">JPG</span></h3> followed by <ul>
        target_section = None

        for h3 in soup.find_all('h3'):
            h3_text = h3.get_text(strip=True).upper()
            if format_preference.upper() in h3_text:
                # Found the right format section
                # Get the following <ul> element
                target_section = h3.find_next_sibling('ul')
                if target_section:
                    logger.info(f"Found {format_preference} section")
                    break

        if not target_section:
            logger.warning(f"Could not find {format_preference} section on page")
            return {}

        # Parse each <li> item for publisher packs
        # Structure: <li>2026.01.14 DC Week (489 MB) :<br>...<a href="...">PIXELDRAIN</a>...</li>
        for li in target_section.find_all('li'):
            li_text = li.get_text(strip=True)

            # Check which publisher this line is for
            for publisher in publishers:
                # Match patterns like "DC Week", "Marvel Week", "Image Week", "INDIE Week"
                publisher_patterns = [
                    rf'\b{re.escape(publisher)}\s*Week\b',
                    rf'\b{re.escape(publisher)}\b.*Week'
                ]

                matched = False
                for pattern in publisher_patterns:
                    if re.search(pattern, li_text, re.IGNORECASE):
                        matched = True
                        break

                if matched:
                    # Found the right publisher, now find the PIXELDRAIN link
                    pixeldrain_link = None

                    for a in li.find_all('a', href=True):
                        href = a.get('href', '')
                        link_text = a.get_text(strip=True).upper()

                        # Check if this is a PIXELDRAIN link
                        # Can be direct pixeldrain.com URL or getcomics.org/dlds/ redirect
                        if 'PIXELDRAIN' in link_text or 'pixeldrain.com' in href.lower():
                            pixeldrain_link = href
                            break
                        # Check for getcomics redirect link with PIXELDRAIN in text
                        elif 'getcomics.org/dlds/' in href.lower() and 'PIXELDRAIN' in link_text:
                            pixeldrain_link = href
                            break

                    if pixeldrain_link:
                        result[publisher] = pixeldrain_link
                        logger.info(f"Found {publisher} {format_preference} link: {pixeldrain_link[:80]}...")
                    else:
                        logger.warning(f"Could not find PIXELDRAIN link for {publisher}")

                    break  # Move to next li item

        logger.info(f"Parsed {len(result)} publisher links from weekly pack")
        return result

    except Exception as e:
        logger.error(f"Error fetching/parsing pack page: {e}")
        return {}
